# Build your own Telemetry client container application

Now that we've understood how the application-hosting infrastructure on IOS-XR works, let's build a Telemetry client/collector application that will run on-box inside a container on the router.  

We select router `r1` as the guinea pig for deploying this client.

While it is perfectly possible to build an LXC container for this application, we choose to create a docker container to take advantage of the automated build workflows that docker offers us.  


## Build Docker images

As part of the lab titled: "Creating your first python Telemetry Collector" in this module, we introduced a sample client that is written in python, connects over gRPC Dial-in and then requests subscription to a GPB encoded Telemetry stream.

As part of this lab, the goal is:  

1. To build a docker image with the same Telemetry client and deploy it on the router `r1`.

2. To package up `pipeline` as a docker image and deploy it as an onbox Telemetry client application.

### Python Telemetry Collector Docker image

#### Create the Dockerfile

To create the `Dockerfile` for the docker image build process, login to the devbox:

<p style="margin: 2em 0!important;padding: 1em;font-family: CiscoSans,Arial,Helvetica,sans-serif;font-size: 1em !important;text-indent: initial;background-color: #e6f2f7;border-radius: 5px;box-shadow: 0 1px 1px rgba(0,127,171,0.25);">**Username**: admin<br/>**Password**: admin<br/>**SSH port**: 2211</p><pre><code>Laptop-terminal:$ ssh -p 2211 admin@10.10.20.170
admin@10.10.20.170's password:
Last login: Sun Aug 26 17:34:49 2018 from 192.168.122.1
admin@devbox:~$
admin@devbox:~$
</code></pre></p>


Create a directory called onbox_telemetry and cd into it:

```
admin@devbox:~$ mkdir onbox_telemetry
admin@devbox:~$ cd onbox_telemetry/
admin@devbox:onbox_telemetry$
admin@devbox:onbox_telemetry$

```

In this directory, create a file called `Dockerfile` (name of the file required by the docker build process).

```bash
admin@devbox:onbox_telemetry$ pwd
/home/admin/onbox_telemetry
admin@devbox:onbox_telemetry$
admin@devbox:onbox_telemetry$ ls -l
total 4
-rw-rw-r-- 1 admin admin 268 Sep 10 00:59 Dockerfile
admin@devbox:onbox_telemetry$
```

which contains the following:

```bash
FROM ubuntu:16.04

RUN apt-get update && apt-get install -y git iproute2 python-pip

RUN pip install grpcio-tools==1.7.0 googleapis-common-protos grpcio==1.7.0 pyyaml

RUN git clone --recursive https://github.com/ios-xr/telemetry-grpc-collectors /root/telemetry-grpc-collectors/

RUN cd /root/telemetry-grpc-collectors/build/python && ./gen-ipv6-nd-bindings.sh

```

This Dockerfile performs the following steps as part of the docker build process:

1. Fetch Ubuntu:16.04 docker image: `FROM ubuntu:16.04`  

2. **Install git, iproute2 and python-pip**: Installs `git` to be able to clone the `telemetry-grpc-collectors` git repository and `iproute2` to allow the application to be run in netns of choice inside the container on IOS-XR (as explained in the previous section).`python-pip` is used to further install all the dependencies for the python Telemetry client/collector to work.

3. Clone the `telemetry-grpc-collectors` git repo  

4. Drop into `build/python` directory and generate the python bindings from the Telemetry .proto files.

#### Build the Docker image

##### Enable `--squash` capability for Docker build  

<div style="margin: 2em 0!important;padding: 1em;font-family: CiscoSans,Arial,Helvetica,sans-serif;font-size: 1em !important;text-indent: initial;background-color: #fdefef;border-radius: 5px;box-shadow: 0 1px 1px rgba(0,127,171,0.25);">
Before we start the build process, let's enable the `--squash` option for the Docker daemon running on the devbox. This is an experimental feature in Docker but highly useful.   
<br/>
Each command specified in the Dockerfile leads to a new filesystem layer in the Docker image. By collapsing the resultant filesystems into a single layer, at the end of the build, the `--squash` option significantly reduces image size.   
**This is highly recommended** to ensure that your docker image does not take up unnecessary disk space on the router.
<br/><br/>
To do this, first create the file `/etc/docker/daemon.json` with the following content:  

<p style="margin: 2em 0!important;padding: 1em;font-family: CiscoSans,Arial,Helvetica,sans-serif;font-size: 1em !important;text-indent: initial;background-color: #e6f2f7;border-radius: 5px;box-shadow: 0 1px 1px rgba(0,127,171,0.25);">
**Note**: sudo password for the `admin` user on devbox is `admin`
</p>


<p><pre><code>
admin@devbox:onbox_slapi$ sudo cat /etc/docker/daemon.json
[sudo] password for admin:
{
  "experimental" : true
}
admin@devbox:onbox_slapi$
admin@devbox:onbox_slapi$

</code></pre></p>


Now, restart the docker daemon:  

<p><pre><code>
admin@devbox:onbox_slapi$
admin@devbox:onbox_slapi$<mark> sudo systemctl restart docker</mark>
admin@devbox:onbox_slapi$
admin@devbox:onbox_slapi$
admin@devbox:onbox_slapi$ <mark>sudo systemctl status docker</mark>
● docker.service - Docker Application Container Engine
   Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
   Active: active (running) since Mon 2018-09-10 01:02:03 PDT; 5s ago
     Docs: https://docs.docker.com
 Main PID: 28668 (dockerd)
    Tasks: 28
   Memory: 39.1M
      CPU: 726ms
   CGroup: /system.slice/docker.service
           ├─28668 /usr/bin/dockerd -H fd://
           ├─28676 docker-containerd --config /var/run/docker/containerd/containerd.toml
           └─28798 docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/23d84604dd5e535d0527002e0517d25dbe888eb10fed7351ff41a9643cfc1cf1 -address /var/run/docker/

Sep 10 01:02:02 devbox dockerd[28668]: time="2018-09-10T01:02:02.930588756-07:00" level=warning msg="Your kernel does not support cgroup rt period"
Sep 10 01:02:02 devbox dockerd[28668]: time="2018-09-10T01:02:02.930629769-07:00" level=warning msg="Your kernel does not support cgroup rt runtime"
Sep 10 01:02:02 devbox dockerd[28668]: time="2018-09-10T01:02:02.937577698-07:00" level=info msg="Loading containers: start."
Sep 10 01:02:03 devbox dockerd[28668]: time="2018-09-10T01:02:03.284739083-07:00" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP addr
Sep 10 01:02:03 devbox dockerd[28668]: time="2018-09-10T01:02:03-07:00" level=info msg="shim docker-containerd-shim started" address="/containerd-shim/moby/23d84604dd5e535d0527002e0517d25dbe888eb10fed7351ff41a9643cfc1cf1/shim.s
Sep 10 01:02:03 devbox dockerd[28668]: time="2018-09-10T01:02:03.789641132-07:00" level=info msg="Loading containers: done."
Sep 10 01:02:03 devbox dockerd[28668]: time="2018-09-10T01:02:03.827351983-07:00" level=info msg="Docker daemon" commit=9ee9f40 graphdriver(s)=overlay2 version=18.03.1-ce
Sep 10 01:02:03 devbox dockerd[28668]: time="2018-09-10T01:02:03.827823950-07:00" level=info msg="Daemon has completed initialization"
Sep 10 01:02:03 devbox systemd[1]: Started Docker Application Container Engine.
Sep 10 01:02:03 devbox dockerd[28668]: time="2018-09-10T01:02:03.853349783-07:00" level=info msg="API listen on /var/run/docker.sock"
admin@devbox:onbox_slapi$
</code></pre></p>

</div>  

#### Issue a Docker build  

Start the docker build process with tag=`telemetry_onbox`

```bash
admin@devbox:onbox_telemetry$ pwd
/home/admin/onbox_telemetry
admin@devbox:onbox_telemetry$ docker build --squash -t telemetry_onbox .
Sending build context to Docker daemon  2.048kB
Step 1/5 : FROM ubuntu:16.04
 ---> b9e15a5d1e1a
Step 2/5 : RUN apt-get update && apt-get install -y git iproute2 python-pip
 ---> Running in b2503ca6ce17
Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Get:2 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]
Get:3 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [90.8 kB]
Get:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [703 kB]
Get:5 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.7 kB]
Get:6 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [468 kB]
Get:7 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3748 B]
Get:8 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Get:9 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Get:10 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]


...........................# OUTPUT SNIPPED #...........................


---> 58c0da5ba545
Step 3/5 : RUN pip install grpcio-tools==1.7.0 googleapis-common-protos grpcio==1.7.0 pyyaml
---> Running in 4634c710327d
Collecting grpcio-tools==1.7.0
 Downloading https://files.pythonhosted.org/packages/0e/c3/d9a9960f12e0bab789da875b1c9a3eb348b51fa3af9544c1edd1f7ef6000/grpcio_tools-1.7.0-cp27-cp27mu-manylinux1_x86_64.whl (21.3MB)
Collecting googleapis-common-protos
 Downloading https://files.pythonhosted.org/packages/00/03/d25bed04ec8d930bcfa488ba81a2ecbf7eb36ae3ffd7e8f5be0d036a89c9/googleapis-common-protos-1.5.3.tar.gz
Collecting grpcio==1.7.0
 Downloading https://files.pythonhosted.org/packages/44/52/e5efd5f7adcfc41967691e296df8b1a96549c8a7f0fa5cf0b23204dcca07/grpcio-1.7.0-cp27-cp27mu-manylinux1_x86_64.whl (5.7MB)
Collecting pyyaml

...........................# OUTPUT SNIPPED #...........................


---> 72141758dc54
Step 4/5 : RUN git clone --recursive https://github.com/ios-xr/telemetry-grpc-collectors /root/telemetry-grpc-collectors/
---> Running in 2ab1a4a21f2a
Cloning into '/root/telemetry-grpc-collectors'...
Submodule 'bigmuddy-network-telemetry-proto' (https://github.com/cisco/bigmuddy-network-telemetry-proto) registered for path 'bigmuddy-network-telemetry-proto'
Cloning into 'bigmuddy-network-telemetry-proto'...
Submodule path 'bigmuddy-network-telemetry-proto': checked out '4419cd20fb73f05d059a37fa3e41fe55f02a528f'
Removing intermediate container 2ab1a4a21f2a
---> 16f0b36de54b
Step 5/5 : RUN cd /root/telemetry-grpc-collectors/build/python && ./gen-ipv6-nd-bindings.sh
---> Running in 75c086978cf3
Generating Python bindings...Done
Removing intermediate container 75c086978cf3
---> be232552d666
Successfully built afde3189cb5a
Successfully tagged telemetry_onbox:latest
admin@devbox:onbox_telemetry$
```

At the end of the successful build, list the docker image by tag `telemetry_onbox`:

```
admin@devbox:onbox_telemetry$ docker images telemetry_onbox
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
telemetry_onbox     latest              afde3189cb5a        4 minutes ago       633MB
admin@devbox:onbox_telemetry$
```

Perfect! The docker image is now ready to be transferred to the router for deployment.


### Pipeline Docker image

The Dockerfile to build pipeline is already available within the git repo:

><https://github.com/cisco/bigmuddy-network-telemetry-pipeline>

specifically, in the `docker/` folder:  

><https://github.com/cisco/bigmuddy-network-telemetry-pipeline/tree/master/docker>

#### Clone the Pipeline git repo

```
admin@devbox:~$ git clone https://github.com/cisco/bigmuddy-network-telemetry-pipeline
Cloning into 'bigmuddy-network-telemetry-pipeline'...
remote: Counting objects: 14615, done.
remote: Total 14615 (delta 0), reused 0 (delta 0), pack-reused 14615
Receiving objects: 100% (14615/14615), 37.72 MiB | 7.21 MiB/s, done.
Resolving deltas: 100% (4014/4014), done.
Checking connectivity... done.
Checking out files: 100% (7867/7867), done.
admin@devbox:~$
```

Hop into the `docker/` directory and dump the Dockerfile contents:

```
admin@devbox:~$ cd bigmuddy-network-telemetry-pipeline/
admin@devbox:bigmuddy-network-telemetry-pipeline$ pwd
/home/admin/bigmuddy-network-telemetry-pipeline
admin@devbox:bigmuddy-network-telemetry-pipeline$
admin@devbox:bigmuddy-network-telemetry-pipeline$ cd docker/
admin@devbox:docker$ ls
Dockerfile  metrics_gpb.json  metrics.json  pipeline  pipeline.conf
admin@devbox:docker$ cat Dockerfile
# How to build?
#
# docker build -f ./Dockerfile -t pipeline:<version> .
#
# How to run?
#
# docker run -d --net=host [--volume <local>:/data] \
#    --name pipeline pipeline:<version>
#
# If using a config section (e.g. gRPC dialin, Influx metrics) with
# user/pass, and do not yet have user/pass credentials in
# configuration file, you will need to replace -d with -ti, and pass
# '-pem <rsa>' RSA key in order to start interactively to provide
# user/pass. (This will generate new config with encryted password
# which can be used in subsequent runs to avoid interactive u/p.)
#
# Command line option --volume data is an option. Without it,
# default config which terminates TCP streams in :5432, and dumps
# to /data/dump.txt will be set up. For any real deployment, a pipeline.conf
# should be provided, so volume should be mounted. If the /data
# volume is mapped locally, the directory must contain pipeline.conf
# to use. If you do need to debug, add the following options at
# the end of run:
#
#   -debug -log=/data/pipeline.log -config=/data/pipeline.conf
#
# How to delete?
#  docker rm -v -f pipeline
#
# Inspecting pipeline.conf or dump.txt?
#  If you mounted these locally with the --volume option, then you can
#  look in the local directory. If not you will need to run "docker
#  inspect pipeline" and find the mount point where you can inspect
#  them.
#
# ----------------------------------------------------

FROM debian:stable-slim

MAINTAINER Christian Cassar <ccassar@cisco.com>

# Stage default configuration, metrics spec and example setup
ADD pipeline.conf /data/pipeline.conf
ADD metrics.json /data/metrics.json
ADD metrics_gpb.json /data/metrics_gpb.json
ADD pipeline /pipeline


VOLUME ["/data"]

WORKDIR /
ENTRYPOINT ["/pipeline"]
CMD ["-log=/data/pipeline.log","-config=/data/pipeline.conf"]
admin@devbox:docker$
```

It can be seen that the Dockerfile for pipeline contains the `ENTRYPOINT` and `CMD` directives which ensure that the `pipeline` process runs by default on `docker run`. Our primary interaction with this container will be through the configuration file (/data/pipeline.conf) that we can mount into it.

But the question arises, how can we run the Pipeline collector in a netns of our choice?
For this, we will need to build a custom Dockerfile of our own with iproute2 installed and the entrypoint modified to accept the netns to run pipeline in.  


#### Create a custom Dockerfile for onbox Pipeline

Drop into the `onbox_telemetry` directory and create a new directory `pipeline` under it:

```
admin@devbox:onbox_telemetry$
admin@devbox:onbox_telemetry$ mkdir pipeline
admin@devbox:onbox_telemetry$ cd pipeline/
admin@devbox:pipeline$


```


##### Create a Custom pipeline.conf for onbox operation
We intend to use gRPC dial-in with an encrypted password in the `pipeline.conf` file.
For this purpose, first create a `pipeline.conf` in this directory with the following content:


<p><pre><code>
admin@devbox:pipeline$ pwd
/home/admin/onbox_telemetry/pipeline
admin@devbox:pipeline$
admin@devbox:pipeline$ cat pipeline.conf
[mymdtrouter]
encoding=gpbkv
server=127.0.0.1:57777
subscriptions=IPV6
tls=false
username=admin
<mark>#password=</mark>

stage=xport_input
type=grpc

[inspector]
datachanneldepth=1000
stage=xport_output
type=tap
file=/root/dump.txt

[default]
id=pipeline
admin@devbox:pipeline$
</code></pre></p>



<div style="margin: 2em 0 !important;padding: 1em;font-family: CiscoSans,Arial,Helvetica,sans-serif;font-size: 1em !important;text-indent: initial;background-color: #e6f2f7;border-radius: 5px;box-shadow: 0 1px 1px rgba(0,127,171,0.25);"><ul><li>The **server information** is encoded as `127.0.0.1:5777` since that is where the gRPC server will be available when we run pipeline on the router.</li> <br/><br/><li>Also notice how the `password` parameter is commented out. Pipeline does not work with cleartext password in the config file. It must be enccrypted using a `pem` file.<br/>We are going to use pipeline's capability to generate an encrypted password for us using a `pem` file so that we can run pipeline on the router directly on `docker run` without the need to enter passwords manually.</li><br/><li>Finally notice the output inspector file which is set to  `/root/dump.txt`. This is intentional, we intend to mount the volume `/misc/app_host/logs` from the Host shell of the router into `/root` of the container so that the resultant logs are available within the Host shell and consequently in the XR LXC shell (since `/misc/app_host` is mounted into the XR lXC from the host) </div>


Use `ssh-keygen`  to create a pem file  under the `/home/admin/onbox_telemetry/pipeline` directory:

```
admin@devbox:pipeline$ ssh-keygen -f pem_file  -t rsa -N ''
Generating public/private rsa key pair.
Your identification has been saved in pem_file.
Your public key has been saved in pem_file.pub.
The key fingerprint is:
SHA256:aeWj/ESnd4ddzmaSa3yhu+F49yO9nWfkmTvItN98h6Q admin@devbox
The key's randomart image is:
+---[RSA 2048]----+
|                 |
|                 |
|          .      |
|         +       |
|        S + .   .|
|       o o + ..Bo|
|        o o +=X+@|
|         o .EO=#@|
|          ...=BB#|
+----[SHA256]-----+
admin@devbox:pipeline$
admin@devbox:pipeline$
admin@devbox:pipeline$ pwd
/home/admin/onbox_telemetry/pipeline
admin@devbox:pipeline$
admin@devbox:pipeline$ ls -l pem_file
-rw------- 1 admin admin 1679 Sep 10 12:36 pem_file
admin@devbox:pipeline$

```

Finally, use pipeline to create a custom pipeline config file that contains an encrypted password using the pem_file created. We use the pre-built binary under the `docker/` folder of the `bigmuddy-network-telemetry-pipeline` we cloned earlier.

On execution, pipeline will prompt you for username and password. Enter the credentials to be used for router `r1`, i.e. `admin/admin`:

<p style="margin: 2em 0 !important;padding: 1em;font-family: CiscoSans,Arial,Helvetica,sans-serif;font-size: 1em !important;text-indent: initial;background-color: #e6f2f7;border-radius: 5px;box-shadow: 0 1px 1px rgba(0,127,171,0.25);">Ignore the "connection error" messages, we are only running pipeline here to generate a new custom config file</p>  

From the output below, it can be seen that a new file was created called `./pipeline.conf_REWRITTEN`.


```
admin@devbox:pipeline$
admin@devbox:pipeline$ /home/admin/bigmuddy-network-telemetry-pipeline/docker/pipeline -pem=./pem_file -config=./pipeline.conf
Startup pipeline
Load config from [./pipeline.conf], logging in [pipeline.log]

CRYPT Client [mymdtrouter],[127.0.0.1:57777]
 Enter username: admin
 Enter password:
Generating sample config...A new configuration file [./pipeline.conf_REWRITTEN] has been written including user name and encrypted password.
In future, you can run pipeline non-interactively.
Do remember to run pipeline with '-pem ./pem_file -config ./pipeline.conf_REWRITTEN' options.
Wait for ^C to shutdown
2018/09/10 12:39:37 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: dial tcp 127.0.0.1:57777: getsockopt: connection refused"; Reconnecting to {127.0.0.1:57777 <nil>}
2018/09/10 12:39:37 Failed to dial 127.0.0.1:57777: context canceled; please retry.
2018/09/10 12:39:38 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: dial tcp 127.0.0.1:57777: getsockopt: connection refused"; Reconnecting to {127.0.0.1:57777 <nil>}
2018/09/10 12:39:38 Failed to dial 127.0.0.1:57777: context canceled; please retry.
2018/09/10 12:39:39 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = "transport: dial tcp 127.0.0.1:57777: getsockopt: connection refused"; Reconnecting to {127.0.0.1:57777 <nil>}
2018/09/10 12:39:39 Failed to dial 127.0.0.1:57777: context canceled; please retry.
^Z
[1]+  Stopped                 /home/admin/bigmuddy-network-telemetry-pipeline/docker/pipeline -pem=./pem_file -config=./pipeline.conf
admin@devbox:pipeline$ kill -9 %1
[1]+  Killed                  /home/admin/bigmuddy-network-telemetry-pipeline/docker/pipeline -pem=./pem_file -config=./pipeline.conf
admin@devbox:pipeline$
admin@devbox:pipeline$


```

Kill the running pipeline process and rename this file to `pipeline_onbox.conf`.

```
admin@devbox:pipeline$
admin@devbox:pipeline$ mv ./pipeline.conf_REWRITTEN ./pipeline_onbox.conf
admin@devbox:pipeline$
```

Dumping the contents of `pipeline_onbox.conf`:

```
[inspector]
file=/root/dump.txt
datachanneldepth=1000
stage=xport_output
type=tap

[default]
id=pipeline

[mymdtrouter]
tls=false
username=admin
stage=xport_input
type=grpc
password=yyy2iJI0dLiy3yVuakEWfFW8klUMM49Qwc2LlXYrrIGGEfJbspNNol2OdihvUsnxRymSvzUNVOiGyIEulQ8z3tNSFweyxJwrZWIrKKIibz7DlOm3/                                                                                                         RPSXFImVG0d1y76yS2u0PgD3zbYB4nL0xFXwfCkQhLHfUjNiVUDTIlOaimG3IULuIHv77un2LXzOaqQKZkso01gS1H6QdD3zBkc32DRq1tK2nPApo2Xm5x16dA7ntFPyD8g+cQ2bemniNl7r1fYiVQEPnIKdTVRhzN9GKcdXurLkbfOgdujxECMIz/TN5T9h4Gssx+                             mUTBJcI348WzHa5Sz38kSQcUadnBVow==
encoding=gpbkv
server=127.0.0.1:57777
subscriptions=IPV6

```

##### Fetch the pipeline binary

Copy over the pipeline binary from the cloned `bigmuddy-network-telemetry-pipeline` repository to `/home/admin/onbox_telemetry/pipeline`:

```
admin@devbox:pipeline$ pwd
/home/admin/onbox_telemetry/pipeline
admin@devbox:pipeline$ cp /home/admin/bigmuddy-network-telemetry-pipeline/docker/pipeline ./
admin@devbox:pipeline$
admin@devbox:pipeline$ ls -l pipeline
-rwxrwxr-x 1 admin admin 74631088 Sep 10 12:52 pipeline
admin@devbox:pipeline$
```


##### Create Dockerfile

Finally, create a `Dockerfile` with the following content in the same directory (`/home/admin/onbox_telemetry/pipeline`)


```
FROM ubuntu:16.04

RUN apt-get update && apt-get install -y git iproute2

VOLUME ["/data"]

ADD ./pipeline /pipeline

ADD pem_file /data/pem_file
ADD pipeline_onbox.conf /data/pipeline.conf

ENV vrf_exec "ip netns exec global-vrf"

CMD $vrf_exec /pipeline -pem=/data/pem_file -log=/data/pipeline.log -config=/data/pipeline.conf
```


#### Issue a Docker Build

```
admin@devbox:pipeline$ docker build --squash -t pipeline_onbox  .
Sending build context to Docker daemon  74.72MB
Step 1/8 : FROM ubuntu:16.04
 ---> b9e15a5d1e1a
Step 2/8 : RUN apt-get update && apt-get install -y git iproute2
 ---> Running in b9621cec7768
Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Get:2 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]
Get:3 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [90.8 kB]
Get:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [703 kB]
Get:5 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.7 kB]
Get:6 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [468 kB]
Get:7 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [3748 B]
Get:8 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Get:9 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]


...........................# OUTPUT SNIPPED #...........................


Step 3/8 : VOLUME ["/data"]
 ---> Using cache
 ---> 479da93c799b
Step 4/8 : ADD ./pipeline /pipeline
 ---> 10736a82d8da
Step 5/8 : ADD pem_file /data/pem_file
 ---> d5cbbfa556c5
Step 6/8 : ADD pipeline_onbox.conf /data/pipeline.conf
 ---> aa28fdbc308c
Step 7/8 : ENV vrf_exec "ip netns exec global-vrf"
 ---> Running in 4e9833344824
Removing intermediate container 4e9833344824
 ---> b3350e6f2595
Step 8/8 : CMD $vrf_exec /pipeline -pem=/data/pem_file -log=/data/pipeline.log -config=/data/pipeline.conf
 ---> Running in eb59da388b00
Removing intermediate container eb59da388b00
 ---> 02ef54e7b38c
Successfully built b421632a1a05
Successfully tagged pipeline_onbox:latest
admin@devbox:pipeline$

```

At the end of the successful build, list the docker image created by tag `pipeline_onbox`


```
admin@devbox:pipeline$ docker images pipeline_onbox
REPOSITORY          TAG                 IMAGE ID            CREATED              SIZE
pipeline_onbox      latest              b421632a1a05        About a minute ago   321MB
admin@devbox:pipeline$
```

Perfect! The docker images are now ready to be transferred to the router for deployment.
